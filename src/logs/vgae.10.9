actual args: {'cpu': False, 'devices': [0], 'model': 'vgae', 'dataset': 'cora', 'local_dataset': False, 'name': 'SelfDefined', 'weighted': False, 'directed': False, 'clf_ratio': 0.9, '_validate': False, '_no_validate': False, 'dim': 128, 'validation_interval': 5, 'debug_output_interval': 5, 'save': True, 'silent': False, 'sparse': False, 'kstep': 4, 'measurement': 'katz', 'table_size': 100000000.0, 'negative_ratio': 5, 'encoder_layer_list': [128], 'nu1': 1e-08, 'nu2': 0.0001, 'decay': False, 'pretrain': False, 'lamb': 0.4, 'path_length': 80, 'num_paths': 10, 'p': 1.0, 'q': 1.0, 'window': 10, 'workers': 8}
output path =  D:\Documents\GitHub\OpenNE\results\VGAE
Loading Cora Dataset from root dir: D:\Documents\GitHub\OpenNE\data\Cora
Executing task UnsupervisedNodeClassification.
Creating test set using 90.0% nodes as training set...finished
Start training...
total iter: 300
epoch 5: train_loss = 0.74327; time used = 1.0783679485321045s
epoch 10: train_loss = 0.80524; time used = 1.0719208717346191s
epoch 15: train_loss = 0.74652; time used = 1.0633032321929932s
epoch 20: train_loss = 0.70411; time used = 1.0829384326934814s
epoch 25: train_loss = 0.66090; time used = 1.0629920959472656s
epoch 30: train_loss = 0.62559; time used = 1.0810503959655762s
epoch 35: train_loss = 0.59027; time used = 1.0647571086883545s
epoch 40: train_loss = 0.56217; time used = 1.0719664096832275s
epoch 45: train_loss = 0.54495; time used = 1.0805423259735107s
epoch 50: train_loss = 0.53574; time used = 1.0799241065979004s
epoch 55: train_loss = 0.52747; time used = 1.0705862045288086s
epoch 60: train_loss = 0.51901; time used = 1.0797958374023438s
epoch 65: train_loss = 0.50984; time used = 1.0678668022155762s
epoch 70: train_loss = 0.50202; time used = 1.0918824672698975s
epoch 75: train_loss = 0.49270; time used = 1.0933001041412354s
epoch 80: train_loss = 0.48501; time used = 1.0557630062103271s
epoch 85: train_loss = 0.47804; time used = 1.0639252662658691s
epoch 90: train_loss = 0.47354; time used = 1.0751357078552246s
epoch 95: train_loss = 0.47053; time used = 1.0728034973144531s
epoch 100: train_loss = 0.46725; time used = 1.1052632331848145s
epoch 105: train_loss = 0.46451; time used = 1.0506069660186768s
epoch 110: train_loss = 0.46223; time used = 1.0819697380065918s
epoch 115: train_loss = 0.45997; time used = 1.1146080493927002s
epoch 120: train_loss = 0.45749; time used = 1.0747897624969482s
epoch 125: train_loss = 0.45537; time used = 1.0906288623809814s
epoch 130: train_loss = 0.45361; time used = 1.1390924453735352s
epoch 135: train_loss = 0.45230; time used = 1.1523356437683105s
epoch 140: train_loss = 0.45119; time used = 1.1479506492614746s
epoch 145: train_loss = 0.45027; time used = 1.1386353969573975s
epoch 150: train_loss = 0.44962; time used = 1.161832332611084s
epoch 155: train_loss = 0.44864; time used = 1.19637131690979s
epoch 160: train_loss = 0.44802; time used = 1.1642389297485352s
epoch 165: train_loss = 0.44725; time used = 1.1421899795532227s
epoch 170: train_loss = 0.44647; time used = 1.156874656677246s
epoch 175: train_loss = 0.44575; time used = 1.141864538192749s
epoch 180: train_loss = 0.44497; time used = 1.1534719467163086s
epoch 185: train_loss = 0.44449; time used = 1.1278889179229736s
epoch 190: train_loss = 0.44363; time used = 1.1415581703186035s
epoch 195: train_loss = 0.44261; time used = 1.1349444389343262s
epoch 200: train_loss = 0.44228; time used = 1.1218092441558838s
epoch 205: train_loss = 0.44157; time used = 1.1238558292388916s
epoch 210: train_loss = 0.44086; time used = 1.1040828227996826s
epoch 215: train_loss = 0.44039; time used = 1.1019387245178223s
epoch 220: train_loss = 0.43968; time used = 1.107215404510498s
epoch 225: train_loss = 0.43944; time used = 1.121795415878296s
epoch 230: train_loss = 0.43906; time used = 1.0929734706878662s
epoch 235: train_loss = 0.43844; time used = 1.1068201065063477s
epoch 240: train_loss = 0.43792; time used = 1.1014704704284668s
epoch 245: train_loss = 0.43715; time used = 1.0940608978271484s
epoch 250: train_loss = 0.43701; time used = 1.1369948387145996s
epoch 255: train_loss = 0.43607; time used = 1.1082231998443604s
epoch 260: train_loss = 0.43571; time used = 1.078841209411621s
epoch 265: train_loss = 0.43511; time used = 1.0905377864837646s
epoch 270: train_loss = 0.43444; time used = 1.101736068725586s
epoch 275: train_loss = 0.43388; time used = 1.0990724563598633s
epoch 280: train_loss = 0.43361; time used = 1.108497142791748s
epoch 285: train_loss = 0.43283; time used = 1.10056471824646s
epoch 290: train_loss = 0.43235; time used = 1.1131470203399658s
epoch 295: train_loss = 0.43186; time used = 1.1086957454681396s
epoch 300: train_loss = 0.43130; time used = 1.0743906497955322s
Finished training. Time used = 66.66749572753906.
Saving embeddings to D:\Documents\GitHub\OpenNE\results\VGAE\VGAE_embeddings.txt...
Saving model to D:\Documents\GitHub\OpenNE\results\VGAE\VGAE_model.txt...
Training classifier using 90.00% nodes...
{'micro': 0.8560885608856088, 'macro': 0.8483555834746311, 'samples': 0.8560885608856088, 'weighted': 0.8542291346873143}
