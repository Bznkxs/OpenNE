actual args: {'cpu': False, 'devices': [0], 'model': 'node2vec', 'dataset': 'blogcatalog', 'local_dataset': False, 'name': 'SelfDefined', 'weighted': False, 'directed': False, 'clf_ratio': 0.9, '_validate': False, '_no_validate': False, 'dim': 128, 'validation_interval': 5, 'debug_output_interval': 5, 'save': True, 'silent': False, 'sparse': False, 'kstep': 4, 'measurement': 'katz', 'table_size': 100000000.0, 'negative_ratio': 5, 'encoder_layer_list': [128], 'nu1': 1e-08, 'nu2': 0.0001, 'decay': False, 'pretrain': False, 'lamb': 0.4, 'path_length': 80, 'num_paths': 10, 'p': 1.0, 'q': 1.0, 'window': 10, 'workers': 8}
output path =  D:\Documents\GitHub\OpenNE\results\Node2vec
Loading BlogCatalog Dataset from root dir: D:\Documents\GitHub\OpenNE\data\BlogCatalog
Executing task UnsupervisedNodeClassification.
Creating test set using 90.0% nodes as training set...finished
Start training...
Preprocess transition probs...
Walk iteration:
Run epoch 0
Epoch 0 ends in 14.101428985595703 seconds.
Run epoch 1
Epoch 1 ends in 14.34549617767334 seconds.
Run epoch 2
Epoch 2 ends in 20.989819765090942 seconds.
Run epoch 3
Epoch 3 ends in 27.50478172302246 seconds.
Run epoch 4
Epoch 4 ends in 25.76935911178589 seconds.
Run epoch 5
Epoch 5 ends in 18.525121450424194 seconds.
Run epoch 6
Epoch 6 ends in 29.266634941101074 seconds.
Run epoch 7
Epoch 7 ends in 18.011427640914917 seconds.
Run epoch 8
Epoch 8 ends in 28.640214443206787 seconds.
Run epoch 9
Epoch 9 ends in 20.35106062889099 seconds.
training Word2Vec model...
Obtaining vectors...
Time used = 225.4120852947235s
Finished training. Time used = 1798.1963529586792.
Saving embeddings to D:\Documents\GitHub\OpenNE\results\Node2vec\Node2vec_embeddings.txt...
Saving model to D:\Documents\GitHub\OpenNE\results\Node2vec\Node2vec_model.txt...
Training classifier using 90.00% nodes...
{'micro': 0.4072802197802198, 'macro': 0.25830525125653025, 'samples': 0.40239689251317157, 'weighted': 0.37742432838110257}
